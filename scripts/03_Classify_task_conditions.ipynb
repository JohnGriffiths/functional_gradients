{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Step 3\n",
    "\n",
    "## Use the projection to gradient space to predict task condition\n",
    "\n",
    "Here we take the reconstructed time series, average within each condition and try to predict the condition in new subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/nibabel/cifti2/parse_cifti2.py:24: FutureWarning: We no longer carry a copy of the 'six' package in nibabel; Please import the 'six' package directly\n",
      "  from ..externals.six import BytesIO\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py as h5\n",
    "\n",
    "from fgrad.predict import features_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#f = h5.File('../data/reconstructed_WM.hdf5')\n",
    "#d_LR = f['Working_memory/LR']\n",
    "#d_RL = f['Working_memory/RL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f = h5.File('/Users/marcel/projects/HCP/volumes_embedded_full.hdf5')\n",
    "d_LR = f['Working_memory/Run1']\n",
    "d_RL = f['Working_memory/Run2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Within-run averaging\n",
    "\n",
    "We take the time points corresponding to each task condition and average them independently within each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels = dict()\n",
    "labels['WM_fix'] = 0\n",
    "labels['WM_0back'] = 1\n",
    "labels['WM_2back'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Block onsets expressed as TRs\n",
    "# We add 6 volumes (4.32 s) to each onset to take into account hemodynamic lag \n",
    "# and additional 4 volumes (2.88 s) to account for instruction\n",
    "nback_LR_2b = np.round(np.array([7.977, 79.369, 150.553, 178.689])/0.72).astype(int)+10\n",
    "nback_LR_0b = np.round(np.array([36.159, 107.464, 221.965, 250.18])/0.72).astype(int)+10\n",
    "nback_RL_2b = np.round(np.array([7.977, 79.369, 178.769, 250.22])/0.72).astype(int)+10\n",
    "nback_RL_0b = np.round(np.array([36.159, 107.464, 150.567, 222.031])/0.72).astype(int)+10\n",
    "nback_fix = np.array([88, 187, 286])+6\n",
    "\n",
    "# Each block lasts for 27.5 seconds\n",
    "vols_2b_LR = np.concatenate([range(x,x+38) for x in nback_LR_2b])\n",
    "vols_0b_LR = np.concatenate([range(x,x+38) for x in nback_LR_0b])\n",
    "vols_2b_RL = np.concatenate([range(x,x+38) for x in nback_RL_2b])\n",
    "vols_0b_RL = np.concatenate([range(x,x+38) for x in nback_RL_0b])\n",
    "vols_fix = np.concatenate([range(x,x+22) for x in nback_fix])\n",
    "vols_fix = np.concatenate([vols_fix, range(395, 405)])\n",
    "\n",
    "# Targets\n",
    "nback_targets_LR = np.zeros(405)\n",
    "nback_targets_LR[vols_2b_LR] = 1\n",
    "nback_targets_LR[vols_fix] = -1\n",
    "\n",
    "nback_targets_RL = np.zeros(405)\n",
    "nback_targets_RL[vols_2b_RL] = 1\n",
    "nback_targets_RL[vols_fix] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:8: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n"
     ]
    }
   ],
   "source": [
    "# Get random group assignments\n",
    "subjects = f['Working_memory/Subjects'][...]\n",
    "np.random.seed(123)\n",
    "sind = np.arange(len(subjects))\n",
    "G1 = sorted(np.random.choice(sind, 100, replace = False ))\n",
    "sind = np.delete(sind,G1)\n",
    "G2 = sorted(np.random.choice(sind, 100, replace = False ))\n",
    "sind = np.delete(sind,G2)\n",
    "G3 = sorted(np.random.choice(sind, 100, replace = False ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "conds = ['WM_fix', 'WM_0back', 'WM_2back']\n",
    "grads = [0,1,2]\n",
    "\n",
    "# Group 1\n",
    "\n",
    "f_WM_train1, t_WM_train1 = features_targets(data = d_LR, subjects = G1, \n",
    "                                      inds = nback_targets_LR, condnames = conds, gradients = grads, labels = labels)\n",
    "\n",
    "f_WM_train2, t_WM_train2 = features_targets(data = d_RL, subjects = G1, \n",
    "                                      inds = nback_targets_RL, condnames = conds, gradients = grads, labels = labels)\n",
    "\n",
    "# Group 2\n",
    "\n",
    "f_WM_train3, t_WM_train3 = features_targets(data = d_LR, subjects = G2, \n",
    "                                      inds = nback_targets_LR, condnames = conds, gradients = grads, labels = labels)\n",
    "\n",
    "f_WM_train4, t_WM_train4 = features_targets(data = d_RL, subjects = G2, \n",
    "                                      inds = nback_targets_RL, condnames = conds, gradients = grads, labels = labels)\n",
    "\n",
    "# Group 3\n",
    "\n",
    "f_WM_test1, t_WM_test1 = features_targets(data = d_LR, subjects = G3, \n",
    "                                      inds = nback_targets_LR, condnames = conds, gradients = grads, labels = labels)\n",
    "\n",
    "f_WM_test2, t_WM_test2 = features_targets(data = d_RL, subjects = G3, \n",
    "                                      inds = nback_targets_RL, condnames = conds, gradients = grads, labels = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classify the task conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Assemble the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f_WM_train = np.vstack([f_WM_train1, f_WM_train2, f_WM_train3, f_WM_train4])\n",
    "t_WM_train = np.concatenate([t_WM_train1, t_WM_train2, t_WM_train3, t_WM_train4])\n",
    "\n",
    "f_WM_test = np.vstack([f_WM_test1, f_WM_test2])\n",
    "t_WM_test = np.concatenate([t_WM_test1, t_WM_test2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Cross-validate to get optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=GroupShuffleSplit(n_splits=100, random_state=0, test_size=0.2, train_size=0.5),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linearsvc', LinearSVC(C=0.25, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'linearsvc__C': array([ 0.01  ,  0.0201, ...,  0.9999,  1.01  ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "groups = np.concatenate([np.ones(600),np.ones(600)+1])\n",
    "\n",
    "p_grid = {\"linearsvc__C\": np.linspace(0.01,1.01,100)}\n",
    "svc = LinearSVC(C=0.25, penalty=\"l1\", dual=False)\n",
    "scl = StandardScaler()\n",
    "svc_z = make_pipeline(scl, svc)\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=100, train_size=0.5, random_state=0)\n",
    "gscv = GridSearchCV(svc_z, param_grid=p_grid, cv=gss, n_jobs = -1)\n",
    "\n",
    "gscv.fit(f_WM_train, t_WM_train, groups = groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Examine the performance on independent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.943333333333\n",
      "Confusion matrix:\n",
      "\n",
      "[[197   3   0]\n",
      " [  2 181  17]\n",
      " [  0  12 188]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.99       200\n",
      "          1       0.92      0.91      0.91       200\n",
      "          2       0.92      0.94      0.93       200\n",
      "\n",
      "avg / total       0.94      0.94      0.94       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print \"Test accuracy:\", gscv.best_estimator_.score(f_WM_test, t_WM_test)\n",
    "print \"Confusion matrix:\\n\\n\", confusion_matrix(t_WM_test, gscv.best_estimator_.predict(f_WM_test))\n",
    "print \"\\n\\nClassification report:\\n\\n\", classification_report(t_WM_test, gscv.best_estimator_.predict(f_WM_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
